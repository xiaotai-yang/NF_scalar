{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-15T06:22:47.984968Z",
     "start_time": "2024-07-15T06:22:46.919971Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "from jax import random\n",
    "from jax.random import PRNGKey, normal\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from jax import vmap\n",
    "from jax.tree import map\n",
    "import time\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def initial_mask(A, L):\n",
    "    A += jnp.transpose(A, (1, 0))\n",
    "    A += A[::-1, :]\n",
    "    A += A[:, ::-1]\n",
    "    A/= 8\n",
    "    return A[:-1, :-1]\n",
    "\n",
    "\n",
    "def backward_propagate_grads(B, L):\n",
    "    # Step 1: Pad the 4x4 gradients to match the 5x5 matrix by adding zeros\n",
    "    A = jnp.pad(B, ((0, 1), (0, 1)), mode='constant')\n",
    "\n",
    "    # Step 2: Reverse the rolling (adjust this step if rolling was applied in your forward function)\n",
    "\n",
    "    # Step 3: Undo the averaging by distributing the gradient contributions\n",
    "    # Since the averaging involved adding the matrix to itself in various transformations,\n",
    "    # we need to create a full 5x5 matrix that sums all these transformations' contributions\n",
    "    A+= jnp.transpose(A, (1, 0))\n",
    "    A+= A[::-1, :]\n",
    "    A+= A[:, ::-1]\n",
    "    A = A/8\n",
    "    return A\n",
    "\n",
    "@jax.jit\n",
    "def phi4_action(phi, m2, lam):\n",
    "    \"\"\"Compute the Euclidean action for the scalar phi^4 theory.\n",
    "\n",
    "    The Lagrangian density is kin(phi) + m2 * phi + l * phi^4\n",
    "\n",
    "    Args:\n",
    "        phi: Single field configuration of shape L^d.\n",
    "        m2: Mass squared term (can be negative).\n",
    "        lam: Coupling constant for phi^4 term.\n",
    "\n",
    "    Returns:\n",
    "        Scalar, the action of the field configuration..\n",
    "    \"\"\"\n",
    "\n",
    "    a = jnp.sum(m2 * phi ** 2)\n",
    "    if lam is not None:\n",
    "        a += jnp.sum(lam * phi ** 4)\n",
    "    # Kinetic term\n",
    "    a += 2*jnp.sum(jnp.array([phi*(phi - jnp.roll(phi, 1, d)/2 - jnp.roll(phi, -1, d)/2)  for d in range(len(phi.shape))]))\n",
    "\n",
    "    return a\n",
    "\n",
    "@jax.jit\n",
    "def diff_phi4_action(phi, m2, lam):\n",
    "    a = 2 * m2 * phi\n",
    "    if lam is not None:\n",
    "        a += 4 * lam * phi ** 3\n",
    "    for d in range(len(phi.shape)):\n",
    "        a += 2*(2*phi - jnp.roll(phi, 1, d) - jnp.roll(phi, -1, d))\n",
    "    return a\n",
    "\n",
    "\n",
    "def compute_ess(logp, logq):\n",
    "    logw = logp - logq\n",
    "    log_ess = 2 * jax.scipy.special.logsumexp(logw, axis=0) - jax.scipy.special.logsumexp(2 * logw, axis=0)\n",
    "    ess_per_cfg = jnp.exp(log_ess) / len(logw)\n",
    "    return ess_per_cfg\n",
    "\n",
    "\n",
    "def normal_pdf(x):\n",
    "    \"\"\"Calculate the PDF of a standard normal distribution.\"\"\"\n",
    "    return (1 / jnp.sqrt(2 * jnp.pi)) * jnp.exp(-0.5 * x ** 2)\n",
    "\n",
    "\n",
    "def get_batch(num_samples, L, seed):\n",
    "    #   Generate a batch of samples from a standard normal distribution.\n",
    "    key = PRNGKey(seed)\n",
    "    x = normal(key, (num_samples, L, L))\n",
    "    logp_x = jnp.sum(jnp.log(normal_pdf(x)), axis=(1, 2))  # log probability of each sample\n",
    "\n",
    "    return (x, logp_x, -x)\n",
    "\n",
    "\n",
    "# @partial(jax.custom_vjp, nondiff_argnums=(0, 1))\n",
    "\n",
    "def W_t(a, t):\n",
    "    b = a.shape[0]//2\n",
    "    return a[0]+jnp.sum(a[1:b+1]*(jnp.sin((jnp.arange(b)+1)*t)).reshape(-1, 1, 1, 1), axis = 0) + jnp.sum(a[b+1:]*(jnp.cos((jnp.arange(b)+1)*t)).reshape(-1, 1, 1, 1), axis = 0)\n",
    "def omega_t(a, t):\n",
    "    b = a.shape[0]//2\n",
    "    return a[0]+jnp.sum(a[1:b+1]*(jnp.sin((jnp.arange(b)+1)*t)).reshape(-1, 1), axis = 0) + jnp.sum(a[b+1:]*(jnp.cos((jnp.arange(b)+1)*t)).reshape(-1, 1), axis = 0)\n",
    "\n",
    "def mul_const_tree(tree, const):\n",
    "    \"\"\"Multiplies every element in the pytree by a constant.\"\"\"\n",
    "    return jax.tree.map(lambda x: x * const, tree)\n",
    "\n",
    "\n",
    "def add_trees(*trees):\n",
    "    \"\"\"Adds multiple pytrees together.\"\"\"\n",
    "    return jax.tree.map(lambda *xs: sum(xs), *trees)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def rk4_odeint(step_size, input_, ts, W_a, omega_a):\n",
    "    def func_(input_, t, W_a, omega_a):\n",
    "        x, logp_x, d_logp_x, diff_xf_W, diff_xf_omega = input_\n",
    "        # jax.debug.print(\"W_a:{}\", W_a.shape)\n",
    "\n",
    "        W = W_t(W_a, t)\n",
    "        omega = omega_a\n",
    "        #omega = omega_t(omega_a, t)\n",
    "        t_k = W_a.shape[0]//2\n",
    "        a_w = jnp.concatenate((jnp.array([1., ]), jnp.sin((jnp.arange(t_k) + 1) * t), jnp.cos((jnp.arange(t_k) + 1) * t)), axis=0)\n",
    "        #a_omega = jnp.concatenate((jnp.array([1., ]), jnp.sin((jnp.arange(t_k) + 1) * t), jnp.cos((jnp.arange(t_k) + 1) * t)), axis=0)\n",
    "        return (\n",
    "        jnp.sum(jnp.fft.ifft2(jnp.fft.fft2(W) * jnp.fft.fft2(jnp.sin(omega.reshape(-1, 1, 1) * x))).real, axis = 0),\n",
    "        jnp.sum(W[:, 0, 0].reshape(-1, 1, 1) * (omega.reshape(-1, 1, 1) * jnp.cos(omega.reshape(-1, 1, 1) * x))),\n",
    "        -jnp.sum(omega.reshape(-1, 1, 1) * jnp.cos(omega.reshape(-1, 1, 1) * x) * jnp.fft.ifft2(\n",
    "        jnp.flip(jnp.roll(jnp.fft.fft2(W), (-1, -1), (-1, -2)), (-1, -2)) * jnp.fft.fft2(d_logp_x)).real,\n",
    "        axis=0) + jnp.sum(W[:, 0, 0].reshape(-1, 1, 1) * ((omega ** 2).reshape(-1, 1, 1) * jnp.sin(omega.reshape(-1, 1, 1) * x)), axis=0),\n",
    "        a_w.reshape(-1, 1, 1, 1) * jnp.sin(omega.reshape(-1, 1, 1) * x),\n",
    "        jnp.fft.ifft2(jnp.fft.fft2(W)*jnp.fft.fft2((x*jnp.cos(omega.reshape(-1, 1, 1)*x)))).real)\n",
    "\n",
    "    \"\"\"Integrate a system of ODEs using the 4th order Runge-Kutta method.\"\"\"\n",
    "\n",
    "    def step_func(cur_y, cur_t, dt):\n",
    "        \"\"\"Take one step of RK4.\"\"\"\n",
    "        k1 = func(cur_y, cur_t, W_a, omega_a)\n",
    "        k2 = func(add_trees(cur_y, mul_const_tree(k1, dt * 0.4)), cur_t + dt * 0.4, W_a, omega_a)\n",
    "        k3 = func(add_trees(cur_y, mul_const_tree(k1, dt * 0.29697761), mul_const_tree(k2, dt * 0.15875964)),\n",
    "                  cur_t + dt * 0.45573725, W_a, omega_a)\n",
    "        k4 = func(add_trees(cur_y, mul_const_tree(k1, dt * 0.21810040), mul_const_tree(k2, -dt * 3.05096516),\n",
    "                            mul_const_tree(k3, dt * 3.83286476)), cur_t + dt, W_a, omega_a)\n",
    "\n",
    "        final_step = add_trees(\n",
    "            mul_const_tree(k1, dt * 0.17476028),\n",
    "            mul_const_tree(k2, -dt * 0.55148066),\n",
    "            mul_const_tree(k3, dt * 1.20553560),\n",
    "            mul_const_tree(k4, dt * 0.17118478)\n",
    "        )\n",
    "        return final_step\n",
    "\n",
    "    def cond_fun(carry):\n",
    "        \"\"\"Check if we've reached the last timepoint.\"\"\"\n",
    "        cur_y, cur_t = carry\n",
    "        return cur_t < ts[1]\n",
    "\n",
    "    def body_fun(carry):\n",
    "        \"\"\"Take one step of RK4.\"\"\"\n",
    "        cur_y, cur_t = carry\n",
    "        next_t = jnp.minimum(cur_t + step_size, ts[1])\n",
    "        dt = next_t - cur_t\n",
    "        dy = step_func(cur_y, cur_t, dt)\n",
    "        return add_trees(cur_y, dy), next_t\n",
    "\n",
    "    func = vmap(func_, in_axes=(0, None, None, None))\n",
    "    init_carry = (input_, ts[0])\n",
    "    y1, t1 = jax.lax.while_loop(cond_fun, body_fun, init_carry)\n",
    "    return y1\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T06:22:48.047644Z",
     "start_time": "2024-07-15T06:22:47.990489Z"
    }
   },
   "id": "f479076147628a1",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ess: 0.008203268\n",
      "itert:  0.8426859378814697\n",
      "Iter: 0, loss: 69.2457\n",
      "ess: 0.0142343715\n",
      "itert:  0.45567822456359863\n",
      "Iter: 10, loss: 57.4776\n",
      "ess: 0.018209998\n",
      "itert:  0.449202299118042\n",
      "Iter: 20, loss: 41.6209\n",
      "ess: 0.056954756\n",
      "itert:  0.455183744430542\n",
      "Iter: 30, loss: 32.4460\n",
      "ess: 0.05758426\n",
      "itert:  0.4626450538635254\n",
      "Iter: 40, loss: 19.2620\n",
      "ess: 0.038252752\n",
      "itert:  0.46324729919433594\n",
      "Iter: 50, loss: 16.5877\n",
      "ess: 0.1534276\n",
      "itert:  0.4538419246673584\n",
      "Iter: 60, loss: 15.7036\n",
      "ess: 0.16600382\n",
      "itert:  0.4609348773956299\n",
      "Iter: 70, loss: 10.5954\n",
      "ess: 0.076813176\n",
      "itert:  0.44864964485168457\n",
      "Iter: 80, loss: 12.8164\n",
      "ess: 0.15171982\n",
      "itert:  0.44385719299316406\n",
      "Iter: 90, loss: 6.7217\n",
      "ess: 0.13309823\n",
      "itert:  0.4446706771850586\n",
      "Iter: 100, loss: 6.1562\n",
      "ess: 0.095971346\n",
      "itert:  0.44431614875793457\n",
      "Iter: 110, loss: 4.4172\n",
      "ess: 0.41132292\n",
      "itert:  0.44417738914489746\n",
      "Iter: 120, loss: 2.7818\n",
      "ess: 0.39055443\n",
      "itert:  0.44135212898254395\n",
      "Iter: 130, loss: 0.9123\n",
      "ess: 0.5527944\n",
      "itert:  0.44364142417907715\n",
      "Iter: 140, loss: -0.4304\n",
      "ess: 0.5997546\n",
      "itert:  0.44060325622558594\n",
      "Iter: 150, loss: -0.7910\n",
      "ess: 0.74894214\n",
      "itert:  0.44202733039855957\n",
      "Iter: 160, loss: -1.2222\n",
      "ess: 0.7448884\n",
      "itert:  0.4419851303100586\n",
      "Iter: 170, loss: -1.2931\n",
      "ess: 0.8103589\n",
      "itert:  0.4364490509033203\n",
      "Iter: 180, loss: -1.4375\n",
      "ess: 0.71871024\n",
      "itert:  0.4788186550140381\n",
      "Iter: 190, loss: -1.4518\n",
      "ess: 0.8475336\n",
      "itert:  0.44223618507385254\n",
      "Iter: 200, loss: -1.3611\n",
      "ess: 0.6476228\n",
      "itert:  0.45098137855529785\n",
      "Iter: 210, loss: -1.3739\n",
      "ess: 0.2592497\n",
      "itert:  0.4675865173339844\n",
      "Iter: 220, loss: -1.3096\n",
      "ess: 0.81111974\n",
      "itert:  0.4695456027984619\n",
      "Iter: 230, loss: -0.5603\n",
      "ess: 0.43571866\n",
      "itert:  0.44085264205932617\n",
      "Iter: 240, loss: -1.3557\n",
      "ess: 0.9291945\n",
      "itert:  0.4412250518798828\n",
      "Iter: 250, loss: -1.5473\n",
      "ess: 0.88735753\n",
      "itert:  0.4569423198699951\n",
      "Iter: 260, loss: -1.5696\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 36\u001B[0m\n\u001B[0;32m     34\u001B[0m diff_xf_W_t0 \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mzeros_like(jnp\u001B[38;5;241m.\u001B[39mrepeat(W_a[\u001B[38;5;28;01mNone\u001B[39;00m, :], num_samples, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m     35\u001B[0m diff_xf_omega_t0 \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mzeros((num_samples, f, L, L))\n\u001B[1;32m---> 36\u001B[0m xf, logp_prob, diff_logp_x, int_diff_xf_W, int_diff_xf_omega \u001B[38;5;241m=\u001B[39m \u001B[43mrk4_odeint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m\u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdlogp_x0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiff_xf_W_t0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiff_xf_omega_t0\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mt0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtf\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW_a\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43momega_a\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m diff_logp_x \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_diff_phi4(xf)\n\u001B[0;32m     39\u001B[0m grad_w \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mmean(jnp\u001B[38;5;241m.\u001B[39mfft\u001B[38;5;241m.\u001B[39mifft2(\n\u001B[0;32m     40\u001B[0m     jnp\u001B[38;5;241m.\u001B[39mflip(jnp\u001B[38;5;241m.\u001B[39mroll(jnp\u001B[38;5;241m.\u001B[39mfft\u001B[38;5;241m.\u001B[39mfft2(int_diff_xf_W), (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), axis\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)), (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)) \u001B[38;5;241m*\u001B[39m jnp\u001B[38;5;241m.\u001B[39mfft\u001B[38;5;241m.\u001B[39mfft2(diff_logp_x)[\n\u001B[0;32m     41\u001B[0m                                                                                          :, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m])\u001B[38;5;241m.\u001B[39mreal,\n\u001B[0;32m     42\u001B[0m                   axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "L = 4\n",
    "lr0 = 0.002\n",
    "lr_schedule = optax.cosine_decay_schedule(\n",
    "    init_value = lr0,\n",
    "    decay_steps = 10000,\n",
    "    alpha = 1e-5\n",
    ")\n",
    "seed = 0\n",
    "train_steps = 500\n",
    "num_samples = 128\n",
    "f = 9\n",
    "metro_samples = 1000\n",
    "t0, tf, dt = 0., 1., 0.02\n",
    "m2 = -1\n",
    "lam = 1\n",
    "t_kernel = 15\n",
    "W_a0 = jnp.zeros((t_kernel, f, L+1, L+1))\n",
    "omega_a = jnp.arange(f)+0.5\n",
    "solver = optimizer = optax.chain(\n",
    "    optax.scale_by_adam(b1=0.8, b2=0.9),  # Use Adam updates to scale the gradients\n",
    "    optax.scale_by_schedule(lr_schedule),  # Apply the learning rate schedule\n",
    "    optax.scale(-1)  # Adam is a minimization algorithm, so we negate the gradients\n",
    ")\n",
    "params = (W_a0, omega_a)\n",
    "opt_state = solver.init(params)\n",
    "batch_phi4 = jax.jit(vmap(partial(phi4_action, m2=m2, lam=lam), in_axes=(0,)))\n",
    "batch_diff_phi4 = jax.jit(vmap(partial(diff_phi4_action, m2=m2, lam=lam), in_axes=(0,)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "    t0_ = time.time()\n",
    "\n",
    "    W_a = jnp.transpose(vmap(vmap(initial_mask, (0, None)), (1, None))(W_a0, L), (1, 0, 2, 3))\n",
    "    x0, logp_x0, dlogp_x0 = get_batch(num_samples, L, seed)\n",
    "    diff_xf_W_t0 = jnp.zeros_like(jnp.repeat(W_a[None, :], num_samples, axis=0))\n",
    "    diff_xf_omega_t0 = jnp.zeros((num_samples, f, L, L))\n",
    "    xf, logp_prob, diff_logp_x, int_diff_xf_W, int_diff_xf_omega = rk4_odeint(dt, (\n",
    "    x0, jnp.zeros(num_samples), dlogp_x0, diff_xf_W_t0, diff_xf_omega_t0), jnp.array([t0, tf]), W_a, omega_a)\n",
    "    diff_logp_x += batch_diff_phi4(xf)\n",
    "    grad_w = jnp.mean(jnp.fft.ifft2(\n",
    "        jnp.flip(jnp.roll(jnp.fft.fft2(int_diff_xf_W), (-1, -1), axis=(-2, -1)), (-2, -1)) * jnp.fft.fft2(diff_logp_x)[\n",
    "                                                                                             :, None, None]).real,\n",
    "                      axis=0)\n",
    "    grad_wa0 = jnp.transpose(vmap(vmap(backward_propagate_grads, (0, None)), (1, None))(grad_w, L), (1, 0, 2, 3))\n",
    "    grad_omega = jnp.mean(jnp.sum(diff_logp_x[:, None] * int_diff_xf_omega, axis=(-2, -1)), axis=0)\n",
    "    logp_xf = logp_x0 - logp_prob\n",
    "    logp = -batch_phi4(xf)\n",
    "    logp_x = logp_xf - logp\n",
    "    loss = logp_x.mean(0)\n",
    "\n",
    "    # print(\"W\", W.shape, \"omega\", omega.shape)\n",
    "    # print(\"grad_w\", grad_w.shape, \"grad_omega\", grad_omega.shape)\n",
    "    updates, opt_state = solver.update((grad_wa0, grad_omega), opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    W_a0, omega_a = params\n",
    "    if i % 10 == 0:\n",
    "        print(\"ess:\", compute_ess(logp, logp_xf))\n",
    "        print(\"itert: \", time.time() - t0_)\n",
    "        print('Iter: {}, loss: {:.4f}\\n'.format(i, loss.item()))\n",
    "        # print(\"grad_w\", grad_w)\n",
    "        # print(\"grad_omega\", grad_omega)\n",
    "    seed += 1\n",
    "    # print(jnp.linalg.norm(W-W_a), jnp.linalg.norm(omega-omega_a))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-15T06:45:08.496806Z",
     "start_time": "2024-07-15T06:43:09.335513Z"
    }
   },
   "id": "9bdccb3590d8bef",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6f4b8e61094734fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
